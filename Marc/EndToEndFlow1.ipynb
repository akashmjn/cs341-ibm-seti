{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import numpy as np\n",
    "import json\n",
    "import pandas as pd\n",
    "import sys\n",
    "import collections\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import ibmseti\n",
    "import os,sys\n",
    "sys.path.append('/home/cs341seti/cs341-ibm-seti/')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ibmseti\n",
    "import collections\n",
    "import scipy.io\n",
    "import matplotlib.pyplot as plt\n",
    "import commonutils as cu\n",
    "import PIL\n",
    "from PIL import Image\n",
    "import model_specs\n",
    "import sklearn\n",
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Gathered the File list store in a data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloading and saving all files to local\n",
    "r = requests.get('https://dal.objectstorage.open.softlayer.com/v1/AUTH_cdbef52bdf7a449c96936e1071f0a46b/simsignals_files/public_list_headers_3april_2017.txt')\n",
    "fileList = r.text.split('\\n')[:-1]\n",
    "fileListJSON = [json.loads(entry) for entry in fileList]\n",
    "# Create a data frame with all the file info\n",
    "fileListTuples = [(str(j['file_name']),str(j['signal_classification']),str(j['uuid'])) for j in fileListJSON]\n",
    "fileListDF = pd.DataFrame.from_records(fileListTuples,columns=[\"file_name\",\"signal_classification\",\"uuid\"])\n",
    "# Creating a file index. Will use this for filenames for easier reference\n",
    "fileListDF['file_index'] = fileListDF.index\n",
    "fileListDF['file_index'] = fileListDF.file_index.apply(lambda x: str.zfill(str(x),6) )\n",
    "## Assigning numbers to classes\n",
    "labelMap = {'noise':0,'brightpixel':1,'narrowband':2,\n",
    "            'narrowbanddrd':3,'squarepulsednarrowband':4,'squiggle':5,'squigglesquarepulsednarrowband':6}\n",
    "fileListDF[\"label\"] = fileListDF[\"signal_classification\"].apply(lambda x: labelMap[x])\n",
    "print labelMap\n",
    "fileListDF.to_csv(\"fileList.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3a. Functionality to download and save the Welch periodogram as a numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function: input - JSON line with url, output - spectrogram array\n",
    "def downloadSaveSignalData(DFrow,base_url,container,data_path):\n",
    "    #********************Configuration Parameters Start************************#\n",
    "    # This is the expected pixel width of the output image, height is determined automatically\n",
    "    desiredSignalWidth = 512\n",
    "    assert desiredSignalWidth&31 == 0 , \"Desired Image width should be a multiple of 32\"\n",
    "    # How much should should the periodogram frequency be scaled by determines periodogram length\n",
    "    periodogramFrequencyScaling = 4\n",
    "    # rotatePSD - accounts for the 90 degree rotation caused by the Welch Periodogram\n",
    "    rotatePsd = True\n",
    "    # Normalize PSD\n",
    "    NormalizePsd = True\n",
    "    # Gauss Filter PSD\n",
    "    GaussFilterPsd = True\n",
    "    # Number of columns eliminated fromthe periodogram to preven the vertical line around the center frequency\n",
    "    numEliminatedColumns = 2\n",
    "    #********************Configuration Parameters End**************************#\n",
    "    # Access file via HTTP method\n",
    "    fname = DFrow['file_name']\n",
    "    print \"\\r{}\".format(DFrow[\"file_index\"]),\n",
    "    r = requests.get('{}/{}/{}'.format(base_url, container, fname))   \n",
    "    aca = ibmseti.compamp.SimCompamp(r.content)\n",
    "    originalSignal = aca.get_signal()\n",
    "    # 2. Get the signal dimensions\n",
    "    rawSignalLength = originalSignal.shape[0]\n",
    "    # 3. Finding the closest multiple of 32 for final image width\n",
    "    desiredSignalWidth = ((desiredSignalWidth)&(~31))\n",
    "    #  This is also the segment length of the periodoogram(num frequency bins)\n",
    "    npersegIn = desiredSignalWidth\n",
    "    # 4. Signal width as determined by the frequency scaling needed for noise rejection\n",
    "    signalWidth = desiredSignalWidth * periodogramFrequencyScaling+numEliminatedColumns\n",
    "    # Finding the scaled signal length to accommodate width as a multiple of desiredSignalWidth*periodogramFrequencyScaling and height as a multiple of 32\n",
    "    scaledSignalLength = signalWidth * ((rawSignalLength/signalWidth)&(~31))\n",
    "    #print(scaledSignalLength)\n",
    "    signalHeight = scaledSignalLength/signalWidth\n",
    "    scaledSignal = originalSignal[0:scaledSignalLength]\n",
    "    # 5. Reshaping the scaled signal\n",
    "    reshapedSignal = aca._reshape(scaledSignal,(signalHeight,signalWidth))\n",
    "    # 6. Computing the periodogoram\n",
    "    from scipy import signal\n",
    "    welchPeriodogram = signal.welch(reshapedSignal,fs= 8000, nperseg=npersegIn,return_onesided=False)\n",
    "    powerSpectralDensity = welchPeriodogram[1]\n",
    "    # 7. Rotating the periodogoram + gauss filtering + normalizing\n",
    "    rotatedPSD = np.hstack((powerSpectralDensity[:,256:(desiredSignalWidth - numEliminatedColumns/2)],powerSpectralDensity[:,numEliminatedColumns/2:desiredSignalWidth/2]))\n",
    "    rotatedPSDGaussFiltered = rotatedPSD\n",
    "    if(GaussFilterPsd):\n",
    "        rotatedPSDGaussFiltered = scipy.ndimage.filters.gaussian_filter(rotatedPSD,sigma=1,order=0,mode=\"nearest\")        \n",
    "    rotatedNormalizedPSDGaussFiltered = rotatedPSDGaussFiltered\n",
    "    if(NormalizePsd):\n",
    "        rotatedNormalizedPSDGaussFiltered = rotatedPSDGaussFiltered/np.linalg.norm(rotatedPSDGaussFiltered)\n",
    "    np.save(\"{}/{}.npy\".format(data_path,DFrow['file_index']),rotatedNormalizedPSDGaussFiltered) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3b. Download all periodogram data, save as a set of numpy arrays (skip if already exists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = 'https://dal.objectstorage.open.softlayer.com/v1/AUTH_cdbef52bdf7a449c96936e1071f0a46b'\n",
    "container = 'simsignals'\n",
    "data_path = \"../data/specdataraw\"\n",
    "### Iterate through fileListDF and save the periodogram of the signal data as .npy file\n",
    "### Use file_index for filename\n",
    "temp = fileListDF.apply(downloadSaveSignalData,axis=1,args=(base_url,container,data_path))\n",
    "del temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Converting all periodograms(numpy files) to images (skip if already exists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#********************Configuration Parameters Start************************#\n",
    "numEliminatedColumns = 2\n",
    "desiredImageWidth = 224\n",
    "desiredImageHeight = 224\n",
    "overwriteFiles = False\n",
    "#********************Configuration Parameters End**************************#\n",
    "specPath = \"../data/specdataraw/\"\n",
    "specfiles = [f for f in os.listdir(specPath) if os.path.isfile(os.path.join(specPath, f))]\n",
    "for i in range(len(specfiles)):\n",
    "    fname = specfiles[i].split(\".\")[0]\n",
    "    savePath = \"../data/specdataimages_{}x{}/\".format(desiredImageWidth,desiredImageHeight)\n",
    "    if(not os.path.isfile('{}{}.jpeg'.format(savePath,fname))):\n",
    "        # Convert and save each file as image\n",
    "        print('Creating {}{}.jpeg\\n'.format(savePath,fname))\n",
    "        spec = np.load(\"{}{}.npy\".format(specPath,fname))\n",
    "        fig, ax = plt.subplots(figsize=(20, 10))\n",
    "        ax.axis('off')\n",
    "        ax.frameon = False\n",
    "        fig.frameon = False\n",
    "        ax.imshow((spec), aspect = 0.5*float(spec.shape[1]) / spec.shape[0],cmap=\"viridis\",interpolation='nearest')\n",
    "        # Saving the plot\n",
    "        print(\"{}{}.png\".format(savePath,fname))\n",
    "        plt.savefig(\"{}{}.png\".format(savePath,fname))\n",
    "        # reopening \n",
    "        from PIL import Image\n",
    "        img = Image.open(\"{}{}.png\".format(savePath,fname))\n",
    "        width = img.size[0]\n",
    "        height = img.size[1]\n",
    "        cropped_img = img.crop((200, 100, width-200, height-100))\n",
    "        cropped_img = cropped_img.resize((desiredImageWidth,desiredImageWidth), Image.LANCZOS)\n",
    "        cropped_img.save('{}{}.jpeg'.format(savePath,fname))\n",
    "        plt.close('all')\n",
    "    else:\n",
    "        print('{}{}.jpeg Exists, skipping \\n'.format(savePath,fname))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Generating activations from images using VGG net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Point to directory\n",
    "# Call model activations function\n",
    "readDir = \"../data/specdataimages_224x224_rgb_periodogram/\"\n",
    "writeDir = \"../data/specdataactivations_224x224_rgb_periodogram/\"\n",
    "layerName = \"block5_pool\"\n",
    "poolfitSize = None\n",
    "cu.datautils.generateAllActivations(readDir,writeDir,layerName,poolfitSize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 6. Creating the dataset from the activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read in all files from directory and combine them into train/val/test datasets\n",
    "readDir = \"../data/specdataactivations_224x224_rgb_periodogram/\"\n",
    "nvalidation = 3000\n",
    "ntest = 1500\n",
    "cu.datautils.createDataset(readDir,\"fileList.csv\",nvalidation,ntest,'../data/activations_224x224.h5',loadImages=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Loading the stored dataset with activations on a subset of classes - 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dim of data: 25088\n",
      "Number of training images = 7171\n",
      "Number of validation images = 1903\n",
      "Number of test images = 913\n",
      "Distribution in training data:\n",
      "0 - 1435\n",
      "1 - 1453\n",
      "2 - 2854\n",
      "3 - 1429\n",
      "Distribution in validation data:\n",
      "0 - 388\n",
      "1 - 362\n",
      "2 - 753\n",
      "3 - 400\n",
      "Distribution in test data:\n",
      "0 - 175\n",
      "1 - 182\n",
      "2 - 388\n",
      "3 - 168\n"
     ]
    }
   ],
   "source": [
    "## Read in all files from directory and combine them into train/val/test datasets\n",
    "# Loading in a dataset with a subset of all classes\n",
    "subsetClasses = {0.0:0.0,2.0:1.0,3.0:2.0,5.0:3.0}\n",
    "dataset = cu.datautils.loadDataset(\"../data/activations_224x224.h5\",subsetClasses=subsetClasses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def runLinSVMModel(dataset,C,nDataset,modeltype,printReports=True,gamma=None):\n",
    "    x_train = dataset['x_train']\n",
    "    y_train = dataset['y_train']\n",
    "    x_test = dataset['x_test']\n",
    "    y_test = dataset['y_test']\n",
    "    \n",
    "    # Scaling training and test data\n",
    "    means = np.mean(x_train,axis=0)\n",
    "    stddev = np.std(x_train,axis=0)\n",
    "    # Preventing zero division\n",
    "    stddev[stddev<1e-3] = 1\n",
    "    x_train = (x_train - means)/stddev\n",
    "    x_test = (x_test - means)/stddev\n",
    "    \n",
    "    if modeltype=='linSVM':\n",
    "        lin_clf = svm.LinearSVC(C=C/nDataset,verbose=True,class_weight='balanced')\n",
    "        lin_clf.fit(x_train, y_train)\n",
    "        pred_train = lin_clf.predict(x_train)\n",
    "        pred_test = lin_clf.predict(x_test)\n",
    "    elif modeltype=='linSVR':\n",
    "        lin_clf = svm.LinearSVC(C=C/nDataset,verbose=True)\n",
    "        lin_clf.fit(x_train, y_train)\n",
    "        pred_train = np.round(lin_clf.predict(x_train))\n",
    "        pred_test = np.round(lin_clf.predict(x_test))\n",
    "    elif modeltype=='rbfSVM':\n",
    "        lin_clf = svm.SVC(C=C/nDataset,gamma=gamma,verbose=True,class_weight='balanced',\n",
    "                          decision_function_shape='ovr')\n",
    "        lin_clf.fit(x_train, y_train)\n",
    "        pred_train = lin_clf.predict(x_train)\n",
    "        pred_test = lin_clf.predict(x_test)\n",
    "\n",
    "    train_report = sklearn.metrics.classification_report(y_train,pred_train)\n",
    "    test_report = sklearn.metrics.classification_report(y_test,pred_test)\n",
    "\n",
    "    train_confmat = sklearn.metrics.confusion_matrix(y_train,pred_train)\n",
    "    test_confmat = sklearn.metrics.confusion_matrix(y_test,pred_test)\n",
    "    \n",
    "    if printReports:\n",
    "        print train_report\n",
    "        print train_confmat\n",
    "        print test_report\n",
    "        print test_confmat\n",
    "\n",
    "        print(\"Classification accuracy: %0.2f\" % sklearn.metrics.accuracy_score(y_test,pred_test) )\n",
    "        print(\"MSE: %0.2f\" % np.mean(np.square(y_test - lin_clf.predict(x_test))) )\n",
    "        print(\"Predictions correlation: %0.2f\") % np.corrcoef(y_test,pred_test,rowvar=0)[0,1]\n",
    "    \n",
    "    result = {'lin_clf':lin_clf,'train_report':train_report,'train_confmat':train_confmat,\n",
    "             'test_report':test_report,'test_confmat':test_confmat,\n",
    "             'train_score':lin_clf.score(x_train,y_train),\n",
    "             'test_score':lin_clf.score(x_test,y_test),\n",
    "             'y_test':y_test,\n",
    "             'pred_test':pred_test}\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the model on this dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear]Results for C=0.798483018903:\n",
      "Train score=0.935294937944   Test score=0.676889375685\n",
      "[LibLinear]Results for C=0.352225523093:\n",
      "Train score=0.853576906986   Test score=0.683461117196\n",
      "[LibLinear]Results for C=0.591126494036:\n",
      "Train score=0.906428671036   Test score=0.672508214677\n",
      "[LibLinear]Results for C=0.0040945648008:\n",
      "Train score=0.671036117696   Test score=0.661555312158\n",
      "[LibLinear]Results for C=0.0103990001293:\n",
      "Train score=0.686096778692   Test score=0.684556407448\n",
      "[LibLinear]Results for C=0.00233495338377:\n",
      "Train score=0.661274578162   Test score=0.658269441402\n",
      "[LibLinear]Results for C=0.144739674261:\n",
      "Train score=0.774926788453   Test score=0.684556407448\n",
      "[LibLinear]Results for C=0.474160998872:\n",
      "Train score=0.884256031237   Test score=0.679079956188\n",
      "[LibLinear]Results for C=0.620149044166:\n",
      "Train score=0.911030539674   Test score=0.672508214677\n",
      "[LibLinear]Results for C=0.866410257968:\n",
      "Train score=0.941988565054   Test score=0.673603504929\n"
     ]
    }
   ],
   "source": [
    "## Loading in and preparing datasets\n",
    "x_train = dataset['x_train']\n",
    "y_train = dataset['y_train']\n",
    "x_val = dataset['x_val']\n",
    "y_val = dataset['y_val']\n",
    "x_test = dataset['x_test']\n",
    "y_test = dataset['y_test']\n",
    "num_val = dataset['x_val'].shape[0]\n",
    "num_test = dataset['x_test'].shape[0]\n",
    "nb_classes = 4\n",
    "## Parameter search for SVM\n",
    "C_values = 10**np.random.uniform(-3,0,10)\n",
    "train_scores = []\n",
    "test_scores = []\n",
    "for C_val in C_values:\n",
    "    a = runLinSVMModel(dataset,C_val,len(dataset['y_train']),printReports=False,modeltype='linSVM')\n",
    "    train_scores.append(a['train_score'])\n",
    "    test_scores.append(a['test_score'])\n",
    "    print \"Results for C={}:\".format(C_val)\n",
    "    print \"Train score={}   Test score={}\".format(a['train_score'],a['test_score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing the failures, using a C value with the best prediction from above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear]Results for C=0.144739674261:\n",
      "Train score=0.774926788453   Test score=0.684556407448\n",
      "[ 2.  3.  0.  1.  2.  3.  2.  0.  1.  2.  2.  2.  3.  2.  1.  3.  0.  1.\n",
      "  2.  0.  2.  2.  2.  1.  0.  1.  2.  2.  0.  0.  1.  3.  0.  2.  2.  0.\n",
      "  2.  1.  1.  2.  3.  3.  1.  0.  2.  2.  0.  1.  2.  2.  1.  3.  1.  3.\n",
      "  2.  2.  2.  2.  0.  3.  2.  2.  0.  1.  2.  0.  3.  1.  1.  2.  3.  2.\n",
      "  2.  1.  2.  0.  2.  2.  2.  3.  2.  1.  1.  0.  2.  2.  1.  1.  2.  0.\n",
      "  2.  1.  2.  3.  2.  2.  0.  0.  2.  3.  0.  1.  2.  2.  0.  2.  3.  2.\n",
      "  3.  2.  2.  1.  0.  1.  2.  2.  2.  0.  2.  3.  1.  2.  2.  3.  3.  0.\n",
      "  1.  2.  0.  2.  1.  3.  3.  0.  0.  1.  3.  2.  2.  1.  3.  1.  2.  2.\n",
      "  2.  2.  3.  3.  1.  1.  2.  0.  3.  1.  1.  1.  1.  2.  0.  3.  2.  3.\n",
      "  0.  3.  2.  0.  2.  1.  0.  0.  1.  1.  3.  3.  2.  1.  2.  2.  1.  1.\n",
      "  2.  0.  2.  3.  2.  2.  0.  2.  3.  1.  0.  1.  0.  3.  0.  0.  2.  2.\n",
      "  3.  3.  2.  2.  0.  0.  2.  3.  1.  1.  3.  1.  3.  0.  3.  2.  0.  3.\n",
      "  2.  1.  3.  2.  2.  1.  0.  3.  2.  3.  2.  0.  3.  2.  2.  2.  1.  0.\n",
      "  3.  0.  3.  2.  2.  2.  0.  2.  0.  0.  0.  0.  0.  1.  2.  2.  0.  3.\n",
      "  2.  2.  3.  2.  0.  2.  2.  2.  3.  3.  2.  2.  0.  2.  3.  1.  2.  0.\n",
      "  2.  1.  2.  1.  2.  0.  2.  3.  3.  2.  1.  0.  3.  1.  0.  1.  1.  1.\n",
      "  2.  0.  0.  2.  0.  0.  2.  1.  2.  1.  1.  3.  0.  2.  2.  0.  0.  0.\n",
      "  2.  2.  2.  2.  1.  1.  2.  1.  0.  3.  2.  3.  2.  1.  2.  1.  0.  3.\n",
      "  2.  2.  2.  3.  3.  2.  0.  2.  0.  1.  1.  2.  3.  2.  2.  3.  2.  0.\n",
      "  2.  2.  2.  2.  0.  2.  1.  2.  1.  3.  2.  2.  2.  3.  0.  3.  1.  2.\n",
      "  1.  2.  1.  2.  2.  2.  1.  1.  0.  2.  3.  1.  2.  1.  2.  3.  2.  3.\n",
      "  0.  2.  2.  0.  2.  1.  2.  2.  0.  1.  0.  2.  3.  2.  2.  3.  2.  3.\n",
      "  1.  2.  1.  2.  2.  3.  0.  2.  0.  2.  2.  1.  2.  1.  2.  3.  1.  1.\n",
      "  2.  2.  0.  2.  0.  0.  2.  1.  0.  2.  1.  0.  2.  2.  0.  2.  3.  2.\n",
      "  3.  2.  2.  2.  1.  1.  2.  0.  1.  2.  2.  3.  2.  3.  2.  2.  1.  2.\n",
      "  1.  2.  0.  2.  1.  1.  3.  2.  3.  2.  3.  1.  3.  2.  2.  2.  2.  2.\n",
      "  3.  2.  0.  0.  3.  2.  2.  2.  3.  3.  3.  3.  2.  1.  0.  1.  1.  0.\n",
      "  2.  2.  3.  3.  1.  1.  2.  1.  3.  3.  2.  2.  0.  3.  2.  3.  0.  0.\n",
      "  2.  2.  3.  3.  2.  2.  2.  3.  2.  2.  2.  2.  1.  0.  2.  3.  2.  0.\n",
      "  1.  0.  0.  2.  3.  2.  3.  2.  2.  0.  2.  2.  1.  0.  0.  3.  0.  3.\n",
      "  1.  2.  3.  2.  3.  0.  1.  2.  1.  1.  2.  2.  1.  2.  3.  0.  0.  1.\n",
      "  1.  2.  3.  2.  0.  2.  3.  3.  2.  2.  2.  3.  0.  2.  2.  2.  2.  2.\n",
      "  0.  1.  2.  0.  2.  2.  3.  0.  2.  2.  2.  1.  0.  3.  2.  3.  1.  2.\n",
      "  2.  2.  0.  2.  2.  1.  2.  1.  3.  1.  1.  1.  2.  2.  3.  3.  2.  3.\n",
      "  0.  2.  2.  0.  1.  2.  2.  2.  2.  3.  2.  3.  1.  2.  2.  1.  0.  0.\n",
      "  1.  2.  1.  1.  3.  2.  3.  0.  0.  2.  2.  0.  2.  2.  2.  3.  1.  2.\n",
      "  2.  1.  1.  3.  1.  3.  2.  1.  2.  3.  2.  1.  0.  3.  1.  3.  1.  0.\n",
      "  2.  2.  2.  3.  0.  0.  1.  3.  2.  1.  3.  2.  3.  3.  0.  2.  0.  2.\n",
      "  0.  2.  2.  1.  0.  0.  2.  2.  0.  2.  0.  2.  1.  3.  3.  3.  0.  2.\n",
      "  2.  0.  2.  3.  1.  2.  3.  0.  3.  3.  1.  2.  1.  2.  0.  2.  1.  2.\n",
      "  2.  1.  2.  0.  2.  1.  2.  3.  3.  2.  2.  1.  0.  2.  0.  0.  2.  0.\n",
      "  3.  2.  0.  2.  3.  1.  2.  2.  1.  3.  2.  2.  2.  0.  2.  3.  2.  2.\n",
      "  0.  1.  3.  2.  2.  0.  2.  0.  1.  3.  2.  3.  1.  0.  0.  2.  2.  2.\n",
      "  2.  1.  2.  3.  2.  2.  0.  2.  0.  1.  1.  3.  3.  0.  2.  0.  0.  0.\n",
      "  2.  2.  1.  2.  2.  2.  1.  2.  2.  2.  2.  1.  2.  0.  2.  3.  0.  1.\n",
      "  1.  1.  3.  2.  0.  2.  2.  2.  3.  1.  1.  2.  0.  2.  2.  2.  2.  2.\n",
      "  1.  2.  3.  0.  2.  2.  2.  2.  3.  3.  2.  3.  2.  3.  2.  1.  2.  2.\n",
      "  1.  3.  1.  0.  2.  1.  2.  0.  0.  0.  2.  1.  2.  1.  2.  1.  0.  0.\n",
      "  2.  2.  3.  2.  2.  2.  0.  0.  2.  0.  2.  2.  3.  0.  0.  2.  2.  2.\n",
      "  3.  1.  0.  2.  0.  1.  1.  2.  1.  1.  1.  2.  1.  1.  3.  3.  2.  2.\n",
      "  1.  3.  1.  2.  0.  2.  1.  3.  3.  1.  0.  2.  2.]\n",
      "[ 0.  0.  0.  1.  2.  0.  2.  0.  1.  0.  2.  2.  0.  2.  1.  3.  0.  0.\n",
      "  2.  0.  2.  2.  1.  1.  0.  1.  0.  1.  0.  0.  1.  0.  0.  1.  1.  0.\n",
      "  2.  1.  1.  2.  0.  3.  1.  0.  0.  1.  0.  2.  1.  2.  0.  3.  1.  3.\n",
      "  2.  2.  1.  3.  0.  3.  1.  2.  0.  1.  0.  0.  3.  1.  1.  0.  0.  1.\n",
      "  2.  0.  0.  0.  0.  0.  2.  3.  0.  0.  1.  3.  2.  0.  1.  1.  2.  0.\n",
      "  2.  1.  0.  0.  1.  2.  0.  0.  2.  3.  0.  0.  0.  2.  0.  0.  3.  1.\n",
      "  3.  2.  1.  1.  0.  1.  0.  0.  2.  0.  0.  3.  3.  2.  2.  3.  0.  0.\n",
      "  1.  2.  0.  2.  1.  0.  3.  0.  0.  2.  0.  2.  2.  0.  3.  1.  0.  0.\n",
      "  2.  1.  3.  0.  1.  1.  2.  0.  3.  1.  0.  1.  1.  0.  0.  3.  2.  0.\n",
      "  0.  1.  1.  0.  0.  2.  0.  0.  0.  1.  3.  0.  2.  1.  0.  1.  1.  1.\n",
      "  2.  0.  2.  3.  2.  1.  0.  1.  0.  0.  0.  1.  0.  3.  0.  0.  0.  2.\n",
      "  0.  0.  3.  0.  0.  0.  2.  0.  1.  1.  3.  1.  3.  0.  3.  2.  0.  3.\n",
      "  1.  1.  3.  0.  2.  1.  0.  3.  0.  3.  1.  0.  3.  0.  1.  0.  1.  0.\n",
      "  0.  0.  3.  2.  2.  2.  0.  1.  0.  0.  0.  0.  0.  0.  2.  3.  0.  0.\n",
      "  2.  2.  3.  2.  0.  2.  2.  2.  3.  3.  1.  1.  0.  2.  3.  1.  2.  0.\n",
      "  2.  1.  1.  0.  2.  0.  2.  3.  3.  2.  1.  0.  0.  2.  0.  1.  0.  1.\n",
      "  3.  0.  0.  1.  0.  0.  0.  1.  2.  1.  1.  3.  0.  2.  2.  0.  0.  0.\n",
      "  2.  2.  2.  2.  1.  1.  2.  0.  0.  0.  0.  3.  2.  1.  0.  0.  0.  3.\n",
      "  2.  2.  1.  3.  0.  0.  0.  2.  0.  1.  1.  2.  0.  2.  0.  0.  0.  0.\n",
      "  1.  2.  2.  0.  0.  2.  1.  0.  0.  0.  0.  2.  2.  3.  0.  3.  0.  0.\n",
      "  2.  2.  1.  2.  1.  1.  0.  1.  0.  2.  0.  0.  0.  1.  0.  3.  2.  3.\n",
      "  0.  0.  1.  0.  2.  1.  2.  2.  0.  1.  0.  2.  3.  2.  2.  3.  0.  3.\n",
      "  0.  1.  1.  1.  2.  3.  0.  2.  0.  2.  2.  0.  2.  1.  2.  3.  1.  1.\n",
      "  2.  2.  0.  2.  0.  0.  1.  1.  0.  0.  2.  0.  1.  2.  0.  2.  0.  3.\n",
      "  3.  0.  2.  2.  1.  1.  0.  0.  1.  1.  2.  3.  2.  3.  2.  1.  0.  2.\n",
      "  1.  2.  0.  0.  0.  1.  3.  2.  3.  2.  3.  1.  3.  2.  0.  0.  2.  2.\n",
      "  3.  1.  0.  0.  3.  1.  2.  1.  0.  2.  3.  0.  1.  1.  0.  0.  1.  0.\n",
      "  2.  2.  3.  3.  0.  1.  2.  1.  3.  3.  1.  2.  0.  3.  1.  0.  0.  3.\n",
      "  1.  0.  0.  2.  2.  0.  2.  0.  0.  2.  1.  1.  0.  0.  2.  3.  0.  0.\n",
      "  0.  0.  3.  2.  3.  2.  0.  0.  0.  0.  2.  2.  1.  0.  0.  3.  0.  3.\n",
      "  1.  1.  3.  2.  3.  0.  1.  2.  1.  1.  1.  2.  1.  1.  3.  0.  0.  0.\n",
      "  0.  2.  3.  3.  0.  2.  3.  3.  2.  2.  0.  0.  0.  1.  0.  0.  2.  2.\n",
      "  0.  1.  2.  0.  2.  2.  0.  0.  2.  2.  2.  1.  0.  0.  1.  3.  1.  2.\n",
      "  2.  2.  0.  0.  2.  1.  1.  1.  0.  1.  1.  0.  0.  2.  3.  3.  2.  0.\n",
      "  0.  2.  0.  0.  0.  2.  2.  2.  2.  3.  2.  3.  1.  0.  2.  1.  0.  0.\n",
      "  1.  2.  0.  2.  0.  2.  3.  0.  0.  1.  2.  0.  2.  2.  0.  0.  0.  1.\n",
      "  0.  1.  1.  0.  1.  0.  0.  1.  2.  3.  1.  1.  0.  3.  0.  3.  1.  0.\n",
      "  0.  1.  2.  3.  0.  0.  1.  3.  2.  1.  3.  1.  3.  3.  0.  1.  0.  0.\n",
      "  0.  2.  2.  1.  0.  0.  1.  2.  0.  2.  0.  0.  3.  0.  0.  3.  0.  2.\n",
      "  1.  0.  0.  3.  1.  2.  3.  0.  3.  0.  1.  0.  1.  0.  0.  1.  1.  1.\n",
      "  2.  1.  2.  3.  1.  1.  2.  0.  3.  3.  0.  0.  0.  1.  0.  0.  2.  0.\n",
      "  3.  2.  0.  2.  3.  2.  2.  1.  1.  3.  2.  0.  0.  0.  1.  3.  0.  2.\n",
      "  0.  0.  3.  2.  1.  0.  1.  0.  1.  3.  2.  3.  1.  0.  0.  2.  2.  1.\n",
      "  2.  0.  2.  0.  0.  1.  0.  2.  0.  1.  1.  3.  0.  0.  2.  0.  0.  0.\n",
      "  2.  0.  2.  1.  0.  2.  1.  2.  0.  2.  2.  1.  0.  0.  2.  3.  0.  1.\n",
      "  1.  1.  3.  0.  0.  1.  2.  0.  3.  1.  1.  0.  0.  1.  0.  2.  2.  2.\n",
      "  1.  2.  3.  0.  0.  2.  1.  1.  0.  3.  0.  3.  2.  0.  0.  1.  0.  2.\n",
      "  1.  0.  1.  0.  2.  1.  2.  0.  0.  0.  2.  2.  1.  1.  1.  1.  0.  0.\n",
      "  1.  2.  3.  2.  2.  0.  0.  0.  2.  0.  2.  2.  3.  0.  0.  2.  2.  0.\n",
      "  0.  0.  0.  1.  0.  0.  1.  0.  1.  1.  1.  2.  0.  1.  3.  3.  2.  2.\n",
      "  1.  3.  2.  1.  0.  2.  1.  3.  0.  1.  0.  2.  2.]\n",
      "['004666' '010324' '006082' '015013' '012977' '004011' '003604' '006069'\n",
      " '007380' '015653' '001032' '006447' '012479' '012353' '015601' '010546'\n",
      " '009892' '014080' '004911' '001420' '001656' '003143' '011404' '010144'\n",
      " '006397' '006462' '012736' '005336' '014208' '015677' '015675' '003912'\n",
      " '011291' '006512' '002926' '006114' '009003' '014372' '002359' '012424'\n",
      " '012401' '012128' '014603' '014685' '006008' '006960' '015631' '002507'\n",
      " '009408' '008234' '006458' '001727' '013991' '007726' '013274' '012915'\n",
      " '006502' '014009' '014047' '000427' '010137' '006842' '010014' '010033'\n",
      " '012852' '014751' '012616' '001740' '001565' '004208' '010726' '015295'\n",
      " '010122' '015256' '004849' '009158' '002725' '009705' '001587' '013306'\n",
      " '004348' '007815' '005518' '006837' '014847' '011077' '010015' '008555'\n",
      " '002664' '000012' '003660' '008786' '012204' '008577' '007613' '006651'\n",
      " '007909' '011527' '012332' '003999' '001866' '009708' '010824' '002499'\n",
      " '012144' '006088' '012059' '013515' '013958' '001513' '004177' '003257'\n",
      " '002160' '003911' '002983' '008576' '005781' '000904' '010760' '005787'\n",
      " '010295' '014046' '002936' '001226' '013026' '010335' '011719' '003647'\n",
      " '013907' '012143' '012766' '011780' '007659' '014814' '008189' '002151'\n",
      " '007144' '010174' '008682' '005362' '008634' '012368' '001241' '008067'\n",
      " '001624' '007393' '011157' '015213' '008664' '003481' '015477' '002250'\n",
      " '005326' '012468' '001392' '009022' '004139' '003294' '001245' '012624'\n",
      " '000526' '001947' '007578' '004328' '011774' '002476' '006738' '014887'\n",
      " '004483' '011113' '013161' '006718' '000594' '004196' '005773' '003201'\n",
      " '011745' '004837' '009320' '000770' '002368' '013559' '005575' '015793'\n",
      " '009416' '001516' '004530' '010594' '004463' '003610' '010588' '008195'\n",
      " '012285' '011717' '013000' '001823' '015466' '004369' '015846' '011041'\n",
      " '002799' '003848' '001310' '008332' '005894' '012297' '002845' '000931'\n",
      " '015632' '008254' '012522' '007250' '014909' '006620' '001763' '011920'\n",
      " '007588' '011681' '003778' '000750' '000300' '001734' '008842' '000006'\n",
      " '003437' '011646' '014321' '003658' '008445' '007225' '003875' '013294'\n",
      " '013336' '014952' '008832' '009803' '013085' '013109' '010576' '002894'\n",
      " '003863' '010974' '013242' '007477' '001742' '005484' '003804' '013396'\n",
      " '012250' '004486' '010694' '011684' '008740' '001286' '005939' '010511'\n",
      " '014696' '002906' '003708' '000909' '007429' '014898' '015786' '000020'\n",
      " '007538' '012715' '004887' '000322' '000345' '003675' '013086' '010093'\n",
      " '009205' '006756' '004021' '000937' '000773' '012276' '013301' '014700'\n",
      " '014275' '011669' '010408' '002020' '009967' '009343' '014223' '002303'\n",
      " '015019' '007422' '006551' '012513' '005962' '015479' '012336' '001566'\n",
      " '012311' '013165' '002388' '003213' '006678' '013933' '002316' '012090'\n",
      " '008114' '014695' '007344' '011280' '007892' '007927' '006198' '009037'\n",
      " '005168' '011405' '012825' '002514' '010600' '013062' '004289' '009737'\n",
      " '001924' '008502' '002575' '011891' '000910' '012021' '008462' '007962'\n",
      " '005891' '015552' '002414' '011886' '010542' '002478' '010647' '015745'\n",
      " '001591' '000455' '001540' '015734' '007827' '000281' '012682' '012506'\n",
      " '011674' '011183' '000590' '012085' '013726' '012108' '006809' '001953'\n",
      " '008187' '012447' '012092' '003818' '008769' '004370' '015565' '002681'\n",
      " '009572' '009355' '012842' '010522' '004116' '007524' '006680' '011832'\n",
      " '012001' '012711' '005378' '014940' '008136' '006189' '007576' '007096'\n",
      " '010020' '014465' '003560' '013466' '003838' '006903' '012910' '009484'\n",
      " '014513' '001171' '011359' '007700' '006845' '002502' '012707' '012929'\n",
      " '013914' '005702' '015237' '008976' '013700' '000223' '015684' '007236'\n",
      " '015609' '011781' '008801' '001104' '003405' '004748' '008628' '007655'\n",
      " '007091' '002235' '008887' '005039' '000624' '012404' '006640' '007767'\n",
      " '001271' '009754' '006056' '003947' '000317' '006717' '008333' '009651'\n",
      " '006708' '005733' '011450' '006905' '012414' '002023' '002178' '003511'\n",
      " '012566' '004080' '005545' '007257' '015701' '013071' '013405' '005507'\n",
      " '012278' '008987' '004819' '012142' '012933' '015844' '001889' '001160'\n",
      " '004163' '006964' '003009' '005956' '007628' '009018' '004847' '001571'\n",
      " '008547' '011888' '007029' '011860' '015943' '008690' '004719' '000162'\n",
      " '007470' '005143' '009702' '003089' '005070' '003564' '006313' '013090'\n",
      " '006822' '001604' '000686' '005211' '015381' '002833' '001431' '014450'\n",
      " '001233' '003694' '004211' '010888' '012943' '001422' '004939' '013920'\n",
      " '004763' '008263' '012811' '012293' '002670' '013745' '006455' '010024'\n",
      " '013683' '008259' '012266' '007915' '015367' '004623' '001447' '008390'\n",
      " '012449' '007893' '004327' '006389' '003517' '014380' '004429' '015469'\n",
      " '003933' '002574' '004693' '006602' '003826' '011439' '010277' '008720'\n",
      " '002798' '004689' '000373' '014453' '007111' '000968' '000046' '014052'\n",
      " '002051' '007964' '004644' '009994' '014612' '001460' '010785' '008959'\n",
      " '008836' '013046' '006664' '008099' '012076' '006467' '015181' '008493'\n",
      " '014192' '000563' '001304' '011140' '013582' '013672' '014084' '009221'\n",
      " '005304' '011963' '012966' '001044' '001330' '014511' '009482' '014191'\n",
      " '003036' '005220' '006613' '005510' '008340' '014768' '009659' '014259'\n",
      " '004492' '007833' '002364' '005988' '008572' '000638' '010160' '006895'\n",
      " '010705' '007790' '009609' '007292' '012180' '014138' '008514' '002630'\n",
      " '002480' '003454' '010075' '002621' '007926' '015772' '000044' '002236'\n",
      " '014643' '011726' '014326' '003068' '015205' '008858' '004481' '007168'\n",
      " '001777' '007252' '003482' '012477' '002186' '000195' '013689' '008523'\n",
      " '010475' '010323' '011420' '009818' '011181' '009756' '004449' '007703'\n",
      " '006174' '007663' '003529' '007672' '010808' '008115' '000657' '001613'\n",
      " '014174' '012638' '006140' '014478' '010827' '002909' '004351' '010169'\n",
      " '013706' '014728' '002052' '010175' '002970' '009074' '007641' '011725'\n",
      " '012165' '002321' '001652' '002610' '011580' '004628' '013915' '013372'\n",
      " '009833' '005441' '000200' '002572' '014455' '004835' '009002' '004609'\n",
      " '014165' '003433' '001298' '002382' '011306' '012416' '015446' '000679'\n",
      " '014877' '001815' '008156' '006004' '013835' '011236' '009978' '011018'\n",
      " '000695' '015524' '003828' '001577' '004272' '000592' '008399' '001929'\n",
      " '010065' '000677' '000041' '000486' '008208' '000154' '009812' '008442'\n",
      " '001957' '004938' '010432' '011080' '009720' '001614' '011240' '005398'\n",
      " '011234' '013970' '009866' '009838' '000802' '014681' '001881' '002916'\n",
      " '014577' '005975' '014776' '003602' '007007' '002121' '002891' '004686'\n",
      " '013248' '002423' '000450' '000459' '010444' '014638' '010666' '006410'\n",
      " '000530' '002841' '011773' '008737' '008975' '004568' '005393' '003176'\n",
      " '008420' '011172' '004016' '011116' '014237' '011045' '011286' '008974'\n",
      " '015643' '006179' '015252' '009943' '003567' '004145' '012641' '006200'\n",
      " '015719' '001899' '007460' '010097' '006883' '011243' '009287' '010630'\n",
      " '012931' '001201' '002289' '000240' '000879' '004099' '011998' '008275'\n",
      " '003212' '010000' '005024' '001851' '015010' '011494' '002162' '001065'\n",
      " '005860' '005249' '001258' '000357' '014910' '003633' '004350' '006648'\n",
      " '004037' '012415' '005237' '014417' '007263' '014550' '000320' '002260'\n",
      " '001220' '011741' '003186' '001066' '010868' '004878' '006892' '009801'\n",
      " '006827' '014654' '002809' '010425' '010648' '015659' '012095' '010972'\n",
      " '010104' '007701' '007619' '013470' '007992' '008680' '008981' '000342'\n",
      " '013671' '010400' '006641' '000636' '010378' '000793' '014335' '008900'\n",
      " '000033' '004638' '003444' '011720' '005752' '002367' '006750' '008916'\n",
      " '015657' '004112' '003208' '011784' '006668' '000913' '006969' '013598'\n",
      " '003773' '008775' '008363' '001546' '007001' '014784' '011763' '006759'\n",
      " '015112' '000512' '000911' '003699' '002115' '013137' '011120' '004641'\n",
      " '010043' '008962' '002573' '013228' '010908' '004090' '008049' '012956'\n",
      " '001177' '015711' '007317' '008079' '005372' '015903' '003563' '015970'\n",
      " '014264' '003884' '010757' '007025' '008785' '000778' '013623' '008323'\n",
      " '014079' '008314' '007269' '000437' '012185' '008428' '002383' '004118'\n",
      " '003998' '000296' '002320' '014060' '008299' '013319' '001434' '010242'\n",
      " '005882' '000857' '008038' '012676' '011846' '001141' '005622' '002988'\n",
      " '009341' '015927' '000945' '012156' '014309' '010435' '001681' '012157'\n",
      " '009625' '005332' '011988' '009276' '012840' '010080' '012531' '011585'\n",
      " '001138']\n",
      "[[171   0   0   4]\n",
      " [ 39 130  11   2]\n",
      " [ 90  79 212   7]\n",
      " [ 53   1   2 112]]\n"
     ]
    }
   ],
   "source": [
    "## Loading in and preparing datasets\n",
    "x_train = dataset['x_train']\n",
    "y_train = dataset['y_train']\n",
    "x_val = dataset['x_val']\n",
    "y_val = dataset['y_val']\n",
    "x_test = dataset['x_test']\n",
    "y_test = dataset['y_test']\n",
    "num_val = dataset['x_val'].shape[0]\n",
    "num_test = dataset['x_test'].shape[0]\n",
    "nb_classes = 4\n",
    "# Using the best C-val from the run above\n",
    "C_val = 0.144739674261\n",
    "train_scores = []\n",
    "test_scores = []\n",
    "a = runLinSVMModel(dataset,C_val,len(dataset['y_train']),printReports=False,modeltype='linSVM')\n",
    "train_scores.append(a['train_score'])\n",
    "test_scores.append(a['test_score'])\n",
    "print \"Results for C={}:\".format(C_val)\n",
    "print \"Train score={}   Test score={}\".format(a['train_score'],a['test_score'])\n",
    "print a['y_test']\n",
    "print a['pred_test']\n",
    "print dataset['test_ids']\n",
    "print (a['test_confmat'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
